{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e40de7b",
   "metadata": {},
   "source": [
    "## Integrating MCP Tool Calls Using MCP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbf0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Load your OPENAI API key from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the chat model with the specified model name\n",
    "llm = init_chat_model(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0c070",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fec6e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found MCP tools: ['square_number', 'add_numbers', 'subtract_numbers', 'multiply_numbers', 'divide_numbers', 'power', 'square_root']\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Creates our LLM with calculator tools\n",
    "async def create_mcp_graph():\n",
    "    # Intialize the MCP client to connect to the calculator server\n",
    "    # MCP (Model Context Protocol) allows LLMs to interact with external tools\n",
    "\n",
    "    client = MultiServerMCPClient({\n",
    "        \"math\": {\n",
    "            \"command\": \"python\", # running a python script\n",
    "            \"args\": [\"calculator_server.py\"], # filepath to the calculator server script\n",
    "            \"transport\": \"stdio\" # communicate via stdin and stdout\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Discover tools from MCP server\n",
    "    mcp_tools = await client.get_tools()\n",
    "    print(f\"Found MCP tools: {[tool.name for tool in mcp_tools]}\")\n",
    "    \n",
    "    # Bind the found tools to the LLM\n",
    "    # Telling the LLM to use these tools when needed\n",
    "    llm_with_tools = llm.bind_tools(mcp_tools)\n",
    "    \n",
    "    # Defining the state structure for our graph\n",
    "    # Messages accumulate as the conversation progresses\n",
    "    class State(TypedDict):\n",
    "        messages: Annotated[list, add_messages]\n",
    "    \n",
    "    # Define the chatbot that processes messages\n",
    "    # Must be async to work with MCP server because of tool calls over network\n",
    "    async def chatbot(state: State):  # ‚Üê Must be async for MCP\n",
    "        return {\"messages\": [await llm_with_tools.ainvoke(state[\"messages\"])]}\n",
    "    \n",
    "    # Create the state graph\n",
    "    graph_builder = StateGraph(State)\n",
    "\n",
    "    # Add the chatbot node that decides what to do\n",
    "    graph_builder.add_node(\"chatbot\", chatbot)\n",
    "    \n",
    "    # Add the tool node that executes the calculator tools\n",
    "    tool_node = ToolNode(tools=mcp_tools)\n",
    "    graph_builder.add_node(\"tools\", tool_node)\n",
    "    \n",
    "    # Conditional routing from chatbot to tools\n",
    "    # If the chatbot decides a tool is needed, it will route to the tools node\n",
    "    # If the tool is not needed, it will end the conversation\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"chatbot\",\n",
    "        tools_condition,\n",
    "        {\"tools\": \"tools\", END: END}\n",
    "    )\n",
    "    \n",
    "    # After the tools execute, return to the chatbot to process the result\n",
    "    graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "    # Start the graph with the initial chatbot node\n",
    "    graph_builder.add_edge(START, \"chatbot\")\n",
    "    \n",
    "    return graph_builder.compile()\n",
    "\n",
    "\n",
    "# Cleanup function to close MCP connections\n",
    "async def cleanup():\n",
    "    \"\"\"Simple cleanup when done with demo\"\"\"\n",
    "    try:\n",
    "        if 'graph' in globals():\n",
    "            # Close any MCP connections if they exist\n",
    "            print(\"üßπ Demo cleanup complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup error: {e}\")\n",
    "\n",
    "# Call when you're done:\n",
    "# await cleanup()\n",
    "\n",
    "\n",
    "# Create the calculator graph by connecting to MCP server\n",
    "graph = await create_mcp_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f92a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def calculate_with_validation(query: str):\n",
    "    \"\"\"Calculate with input validation and better error handling\"\"\"\n",
    "\n",
    "    # Check if the query is meaningful\n",
    "    if not query or len(query.strip()) < 3:\n",
    "        return \"Please provide a valid math question.\"\n",
    "    \n",
    "    try:\n",
    "        # Invoke the graph with a system message to guid the LLM and user message with the query\n",
    "        result = await graph.ainvoke({\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    # system message instructs the LLM on how to behave\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a precise calculator. Follow order of operations strictly. Use parentheses to clarify operations. Always calculate step by step.\"\n",
    "                },\n",
    "                {\n",
    "                    # user message contains the math query\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": query\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        # Return the result from the LLM\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Calculation error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5da4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "result = await calculate_with_validation(\"what's the square root of 156.789, plus 47.234, then multiply by 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91213a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The result of the calculation \\(((\\sqrt{156.789} + 47.234) \\times 3)\\) is approximately \\(179.27\\).\n"
     ]
    }
   ],
   "source": [
    "# Viewing the answer\n",
    "# Output is an AIMessage\n",
    "# Accessing the last message content for the final answer\n",
    "print(\"Result:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1997e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the tool flow\n",
    "def quick_tool_flow(result):\n",
    "    \"\"\"Quick tool flow view with each call on a new line.\"\"\"\n",
    "    \n",
    "    # getting all messages from the conversation result\n",
    "    messages = result[\"messages\"]\n",
    "    \n",
    "    flow = []\n",
    "\n",
    "    # go through each message and check for tool calls\n",
    "    for message in messages:\n",
    "        # checking tool call\n",
    "        if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "            for call in message.tool_calls:\n",
    "                flow.append(f\"üîß {call['name']}({call['args']})\")\n",
    "        # checking tool response\n",
    "        elif hasattr(message, 'name') and hasattr(message, 'content'):\n",
    "            flow.append(f\"‚Üí {message.content}\")\n",
    "    \n",
    "    print(\"TOOL CALL FLOW:\")\n",
    "    for step in flow:\n",
    "        print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc383131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL FLOW:\n",
      "‚Üí You are a precise calculator. Follow order of operations strictly. Use parentheses to clarify operations. Always calculate step by step.\n",
      "‚Üí what's the square root of 156.789, plus 47.234, then multiply by 3?\n",
      "üîß square_root({'number': 156.789})\n",
      "‚Üí 12.521541438656824\n",
      "üîß add_numbers({'a': 12.521541438656824, 'b': 47.234})\n",
      "‚Üí 59.75554143865683\n",
      "üîß multiply_numbers({'a': 59.75554143865683, 'b': 3})\n",
      "‚Üí 179.2666243159705\n",
      "‚Üí The result of the calculation \\(((\\sqrt{156.789} + 47.234) \\times 3)\\) is approximately \\(179.27\\).\n"
     ]
    }
   ],
   "source": [
    "# Visualize the tool flow\n",
    "quick_tool_flow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca72a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Demo cleanup complete\n"
     ]
    }
   ],
   "source": [
    "await cleanup()  # Call cleanup when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa27d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
