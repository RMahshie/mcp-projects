{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e40de7b",
   "metadata": {},
   "source": [
    "## Integrating MCP Tool Calls Using MCP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbf0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Load your OPENAI API key from the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Initialize the chat model with the specified model name\n",
    "llm = init_chat_model(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0c070",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec6e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found MCP tools: ['square_number', 'add_numbers', 'subtract_numbers', 'multiply_numbers', 'divide_numbers', 'power', 'square_root']\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Optional, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "async def create_mcp_graph():\n",
    "    # Connect to MCP servers\n",
    "    client = MultiServerMCPClient({\n",
    "        \"math\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"calculator_server.py\"], \n",
    "            \"transport\": \"stdio\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Discover tools from MCP server\n",
    "    mcp_tools = await client.get_tools()\n",
    "    print(f\"Found MCP tools: {[tool.name for tool in mcp_tools]}\")\n",
    "    \n",
    "    # Build the graph with MCP tools\n",
    "    llm_with_tools = llm.bind_tools(mcp_tools)\n",
    "    \n",
    "    class State(TypedDict):\n",
    "        messages: Annotated[list, add_messages]\n",
    "        structured_output: Optional[str]\n",
    "    \n",
    "    async def chatbot(state: State):  # ‚Üê Must be async for MCP\n",
    "        return {\"messages\": [await llm_with_tools.ainvoke(state[\"messages\"])]}\n",
    "    \n",
    "    \n",
    "    async def format_result(state: State):\n",
    "        \"\"\"Format the conversation into structured output\"\"\"\n",
    "        \n",
    "        class MathResult(BaseModel):\n",
    "            calculation: str = Field(description=\"The mathematical expression that was calculated\")\n",
    "            result: float = Field(description=\"The result of the calculation\")\n",
    "            steps: List[str] = Field(description=\"Step by step breakdown of the calculation\")\n",
    "            confidence: str = Field(description=\"Confidence level of the result\")\n",
    "        \n",
    "        # Create a summary of the conversation\n",
    "        conversation_parts = []\n",
    "        for msg in state[\"messages\"]:  # Don't reverse, keep chronological order\n",
    "            msg_type = getattr(msg, 'type', 'unknown')\n",
    "        \n",
    "            if hasattr(msg, 'content') and msg.content:\n",
    "                conversation_parts.append(f\"{msg_type}: {msg.content}\")\n",
    "        \n",
    "            # Include tool calls information\n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                for call in msg.tool_calls:\n",
    "                    conversation_parts.append(f\"{msg_type}_tool_call: {call['name']}({call['args']})\")\n",
    "        \n",
    "            # Include tool results\n",
    "            if hasattr(msg, 'tool_call_id'):\n",
    "                tool_name = getattr(msg, 'name', 'unknown_tool')\n",
    "                conversation_parts.append(f\"tool_result_{tool_name}: {msg.content}\")\n",
    "    \n",
    "        conversation_summary = \"\\n\".join(conversation_parts)\n",
    "        \n",
    "        structured_llm = llm.with_structured_output(MathResult)\n",
    "        \n",
    "        structured_output = await structured_llm.ainvoke([{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"extract structured math result from this conversation: {conversation_summary}\"\n",
    "            }])\n",
    "        \n",
    "        json_output = structured_output.model_dump() # convert to json\n",
    "        \n",
    "        import json\n",
    "        return {\"structured_output\": json.dumps(json_output, indent=2)}\n",
    "\n",
    "    \n",
    "\n",
    "    graph_builder = StateGraph(State)\n",
    "    graph_builder.add_node(\"chatbot\", chatbot)\n",
    "    \n",
    "    tool_node = ToolNode(tools=mcp_tools)\n",
    "    graph_builder.add_node(\"tools\", tool_node)\n",
    "    \n",
    "    graph_builder.add_node(\"format_result\", format_result)\n",
    "\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"chatbot\",\n",
    "        tools_condition,\n",
    "        {\"tools\": \"tools\", END: \"format_result\"}\n",
    "    )\n",
    "\n",
    "    graph_builder.add_edge(START, \"chatbot\")\n",
    "    graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "    graph_builder.add_edge(\"format_result\", END)\n",
    "    \n",
    "    return graph_builder.compile()\n",
    "\n",
    "\n",
    "# Cleanup function to close MCP connections\n",
    "async def cleanup():\n",
    "    \"\"\"Simple cleanup when done with demo\"\"\"\n",
    "    try:\n",
    "        if 'graph' in globals():\n",
    "            # Close any MCP connections if they exist\n",
    "            print(\"Demo cleanup complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup error: {e}\")\n",
    "\n",
    "# Call when you're done:\n",
    "# await cleanup()\n",
    "\n",
    "\n",
    "# CREATE the graph first\n",
    "graph = await create_mcp_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f92a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def calculate_with_validation(query: str):\n",
    "    \"\"\"Calculate with input validation and better error handling\"\"\"\n",
    "\n",
    "    # Check if the query is meaningful\n",
    "    if not query or len(query.strip()) < 3:\n",
    "        return \"Please provide a valid math question.\"\n",
    "    \n",
    "    try:\n",
    "        print(\"Asking a math question...\")\n",
    "        result = await graph.ainvoke({\n",
    "            \"messages\": [\n",
    "                {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful math assistant that can perform complex calculations using external tools.\\n\"\n",
    "                \"           Use the provided tools one at a time to solve the problem.\\n\"\n",
    "                \"           Never use the tools in parallel, always wait for the result of one tool before using another.\\n\"\n",
    "                \"           Follow order of operations strictly.\\n\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": query + \"\\n\",\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # Return the result from the LLM\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Calculation error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5da4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking a math question...\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "result = await calculate_with_validation(\"what's the square root of 156.789, plus 47.234, then multiply by 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91213a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final answer: The result of the calculation is approximately \\(179.27\\).\n"
     ]
    }
   ],
   "source": [
    "# Viewing the answer\n",
    "# Output is an AIMessage\n",
    "# Accessing the last message content for the final answer\n",
    "print(\"Final answer:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9982f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output: {\n",
      "  \"calculation\": \"((sqrt(156.789) + 47.234) * 3)\",\n",
      "  \"result\": 179.2666243159705,\n",
      "  \"steps\": [\n",
      "    \"Calculate the square root of 156.789 to get 12.521541438656824.\",\n",
      "    \"Add the result to 47.234 to get 59.75554143865683.\",\n",
      "    \"Multiply the result by 3 to get 179.2666243159705.\"\n",
      "  ],\n",
      "  \"confidence\": \"High\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Viewing the structured output\n",
    "print(\"Structured output:\", result[\"structured_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca72a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Demo cleanup complete\n"
     ]
    }
   ],
   "source": [
    "await cleanup()  # Call cleanup when done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
